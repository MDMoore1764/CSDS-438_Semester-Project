{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's design the VQ VAE layers, since this is not a part of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: inspired by https://keras.io/examples/generative/vq_vae/\n",
    "# Paper: https://arxiv.org/abs/1711.00937\n",
    "class VectorQuantizer(keras.layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping the embedding_dim intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs**2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings**2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the VQ VAE is defined, we need to define our encoder and decoder. This can be any model for encoding and decoding. OpenAI's JukeBox project uses noncausal 1-D dilated convolutions, interleaved with downsampling and upsampling 1-D convolutions. So let's use this approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(input_shape, conv_filters, latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = keras.layers.Conv1D(conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        encoder_inputs\n",
    "    )\n",
    "    \n",
    "    x = keras.layers.Conv1D(2 * conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    \n",
    "    encoder_outputs = keras.layers.Conv1D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(input_shape, conv_filters, latent_dim):\n",
    "    latent_inputs = keras.Input(shape=get_encoder(input_shape, conv_filters, latent_dim).output.shape[1:])\n",
    "    \n",
    "    x = keras.layers.Conv1DTranspose(\n",
    "        2 * conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\"\n",
    "    )(latent_inputs)\n",
    "    \n",
    "    x = keras.layers.Conv1DTranspose(\n",
    "        conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\"\n",
    "    )(x)\n",
    "    \n",
    "    decoder_outputs = keras.layers.Conv1DTranspose(1, 3, padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "def get_vqvae(input_shape, num_embeddings, conv_filters, latent_dim):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    encoder = get_encoder(input_shape, conv_filters, latent_dim)\n",
    "    decoder = get_decoder(input_shape, conv_filters, latent_dim)\n",
    "    \n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load from raw wav data and test our network's ability to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 44100, 65535)\n",
      "44100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_size = 2 ** 16 - 1\n",
    "\n",
    "def load_song_file(song_file: str):\n",
    "    rate, song_data = wavfile.read(song_file)\n",
    "\n",
    "    mono_data = song_data\n",
    "\n",
    "    if song_data.shape[1] == 2:\n",
    "        mono_data = np.average(song_data, axis=1)\n",
    "        \n",
    "    audio = (mono_data / np.max(mono_data))\n",
    "    audio = (audio * embedding_size) - 2 ** 15\n",
    "    audio = audio.astype(np.int16)\n",
    "    return rate, audio\n",
    "        \n",
    "        \n",
    "def get_training_sequences(data, rate, chunk_duration):\n",
    "    chunk_size = int(rate * chunk_duration)\n",
    "    \n",
    "    Xs = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data[i:i + chunk_size]\n",
    "        chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode='constant')\n",
    "        chunk = tf.one_hot(chunk, depth=embedding_size, dtype=tf.int16)\n",
    "        Xs.append(chunk)\n",
    "        # Ys.append(chunk[-1])\n",
    "        \n",
    "        \n",
    "    X = np.array(Xs, dtype=np.int16)\n",
    "    # Y = np.array(Ys)\n",
    "    \n",
    "    \n",
    "    return X\n",
    "\n",
    "rate = 44_100\n",
    "sample_song = \"data/Wavs/tvari-tokyo-cafe-159065.wav\"\n",
    "_, data = load_song_file(sample_song)\n",
    "\n",
    "test_duration = 10\n",
    "sequence_duration = 1\n",
    "set_samples = int(sequence_duration * rate)\n",
    "data = data[:set_samples * test_duration]\n",
    "\n",
    "\n",
    "X = get_training_sequences(data, rate, sequence_duration)\n",
    "\n",
    "print(X.shape)\n",
    "print(set_samples)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vq_vae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 44100, 65535)]    0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 11025, 32)         6299680   \n",
      "                                                                 \n",
      " vector_quantizer (VectorQu  (None, 11025, 32)         2097120   \n",
      " antizer)                                                        \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 44100, 1)          12481     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8409281 (32.08 MB)\n",
      "Trainable params: 8409281 (32.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 32\n",
    "latent_dim = 32\n",
    "n_batch = 32\n",
    "\n",
    "vaqae = get_vqvae((set_samples, embedding_size), embedding_size, conv_filters, latent_dim)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "vaqae.compile(optimizer=optimizer, loss=loss, metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "vaqae.summary()\n",
    "\n",
    "# epochs = 1\n",
    "# x_train = X[:n_train_size]\n",
    "# vaqae.fit(x_train, x_train, batch_size=n_batches, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, input_shape, conv_filters = 32, latent_dim=32, num_embeddings=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        # self.input_shape = input_shape\n",
    "        # self.conv_filters = conv_filters\n",
    "\n",
    "        self.vqvae = get_vqvae(input_shape, self.num_embeddings, conv_filters, self.latent_dim)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.checkpoint = tf.train.Checkpoint(optimizer=self.optimizer, model=self.vqvae)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, './checkpoints/Waveforms_with_VQVAE', checkpoint_name=\"checkpoint\", max_to_keep=5)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }\n",
    "        \n",
    "        \n",
    "conv_filters = 32\n",
    "latent_dim = 32\n",
    "n_batch = 32\n",
    "input_shape = (set_samples, embedding_size)\n",
    "\n",
    "\n",
    "# x_train = X[:n_train_size * n_batch]\n",
    "\n",
    "# trainer = VQVAETrainer(1, input_shape, conv_filters, latent_dim, embedding_size)\n",
    "\n",
    "# for epoch in  tqdm(range(epochs), \"Epoch\"):\n",
    "#     for i in tqdm(range(0, x_train.shape[0], n_batch), \"Batch\"):\n",
    "        \n",
    "#         step_x = np.array(x_train[i:n_batch])\n",
    "#         result = trainer.train_step(step_x)\n",
    "        \n",
    "#     # Checkpoints\n",
    "#     trainer.checkpoint_manager.save()\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit_vqvae(data, batch_size, epochs):\n",
    "    trainer = VQVAETrainer(0.2, input_shape, conv_filters, latent_dim, embedding_size)\n",
    "    \n",
    "    for _ in  range(epochs):\n",
    "        for i in tqdm(range(0, data.shape[0], batch_size), \"Batch\"):\n",
    "            step_x = data[i: i + batch_size]\n",
    "            result = trainer.train_step(step_x)\n",
    "            for key, value in result.items():\n",
    "                print(f\"{key}: {value}\", sep = \" | \", end=\"\")\n",
    "\n",
    "        # Checkpoints\n",
    "        trainer.checkpoint_manager.save()\n",
    "        \n",
    "        \n",
    "    return trainer.vqvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:   0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 108. GiB for an array with shape (10, 44100, 65535) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# n_train_size = 3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# x_train = X[:n_train_size * n_batch]   \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfit_vqvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 86\u001b[0m, in \u001b[0;36mfit_vqvae\u001b[1;34m(data, batch_size, epochs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     85\u001b[0m     step_x \u001b[38;5;241m=\u001b[39m data[i: i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m---> 86\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36mVQVAETrainer.train_step\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# Outputs from the VQ-VAE.\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m         reconstructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvqvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# Calculate the losses.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         reconstruction_loss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     38\u001b[0m             tf\u001b[38;5;241m.\u001b[39mreduce_mean((x \u001b[38;5;241m-\u001b[39m reconstructions) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_variance\n\u001b[0;32m     39\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:86\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     83\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     84\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     85\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 108. GiB for an array with shape (10, 44100, 65535) and data type float32"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "# n_train_size = 3\n",
    "\n",
    "# x_train = X[:n_train_size * n_batch]   \n",
    "model = fit_vqvae(X, n_batch, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VQVAETrainer(0.2, input_shape, conv_filters, latent_dim, embedding_size)\n",
    "trainer.checkpoint.restore(trainer.checkpoint_manager.latest_checkpoint)\n",
    "\n",
    "model = trainer.vqvae\n",
    "\n",
    "\n",
    "# vqvae = get_vqvae(input_shape, embedding_size, conv_filters, latent_dim)\n",
    "\n",
    "# checkpoint = tf.train.Checkpoint(model=vqvae)\n",
    "# # manager = tf.train.CheckpointManager(checkpoint, './checkpoints/Waveforms_with_VQVAE', checkpoint_name=\"checkpoint\", max_to_keep=5)\n",
    "\n",
    "# # latest_checkpoint = manager.restore_or_initialize()\n",
    "\n",
    "# status = checkpoint.restore('./checkpoints/Waveforms_with_VQVAE/')\n",
    "\n",
    "# status.assert_consumed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
