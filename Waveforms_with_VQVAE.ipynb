{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's design the VQ VAE layers, since this is not a part of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VectorQuantizer(keras.layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs**2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings**2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the VQ VAE is defined, we need to define our encoder and decoder. This can be any model for encoding and decoding. OpenAI's JukeBox project uses noncausal 1-D dilated convolutions, interleaved with downsampling and upsampling 1-D convolutions. So let's use this approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(input_shape, conv_filters, latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = keras.layers.Conv1D(conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        encoder_inputs\n",
    "    )\n",
    "    \n",
    "    x = keras.layers.Conv1D(2 * conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    \n",
    "    encoder_outputs = keras.layers.Conv1D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(input_shape, conv_filters, latent_dim):\n",
    "    latent_inputs = keras.Input(shape=get_encoder(input_shape, conv_filters, latent_dim).output.shape[1:])\n",
    "    \n",
    "    x = keras.layers.Conv1DTranspose(\n",
    "        2 * conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\"\n",
    "    )(latent_inputs)\n",
    "    \n",
    "    x = keras.layers.Conv1DTranspose(\n",
    "        conv_filters, 3, activation=\"relu\", strides=2, padding=\"same\"\n",
    "    )(x)\n",
    "    \n",
    "    decoder_outputs = keras.layers.Conv1DTranspose(1, 3, padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "def get_vqvae(input_shape, num_embeddings, conv_filters, latent_dim):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    encoder = get_encoder(input_shape, conv_filters, latent_dim)\n",
    "    decoder = get_decoder(input_shape, conv_filters, latent_dim)\n",
    "    \n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load from raw wav data and test our network's ability to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_size = 2 ** 16 - 1\n",
    "\n",
    "def load_song_file(song_file: str):\n",
    "    rate, song_data = wavfile.read(song_file)\n",
    "\n",
    "    mono_data = song_data\n",
    "\n",
    "    if song_data.shape[1] == 2:\n",
    "        mono_data = np.average(song_data, axis=1)\n",
    "        \n",
    "    audio = (mono_data / np.max(mono_data))\n",
    "    audio = (audio * embedding_size) - 2 ** 15\n",
    "    audio = audio.astype(np.int16)\n",
    "    return rate, audio\n",
    "        \n",
    "        \n",
    "def get_training_sequences(data, rate, chunk_duration):\n",
    "    chunk_size = int(rate * chunk_duration)\n",
    "    \n",
    "    Xs = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data[i:i + chunk_size]\n",
    "        chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode='constant')\n",
    "        chunk = tf.one_hot(chunk, depth=embedding_size)\n",
    "        Xs.append(chunk)\n",
    "        # Ys.append(chunk[-1])\n",
    "        \n",
    "        \n",
    "    X = np.array(Xs)\n",
    "    # Y = np.array(Ys)\n",
    "    \n",
    "    \n",
    "    return X\n",
    "\n",
    "sample_song = \"data/Wavs/tvari-tokyo-cafe-159065.wav\"\n",
    "rate, data = load_song_file(sample_song)\n",
    "\n",
    "test_duration = 5\n",
    "data = data[:int(test_duration * rate)]\n",
    "\n",
    "\n",
    "X = get_training_sequences(data, rate, 1)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filters = 32\n",
    "latent_dim = 32\n",
    "n_batch = 32\n",
    "\n",
    "vaqae = get_vqvae((rate, embedding_size), embedding_size, conv_filters, latent_dim)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "vaqae.compile(optimizer=optimizer, loss=loss, metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "vaqae.summary()\n",
    "\n",
    "# epochs = 1\n",
    "# x_train = X[:n_train_size]\n",
    "# vaqae.fit(x_train, x_train, batch_size=n_batches, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, input_shape, conv_filters = 32, latent_dim=32, num_embeddings=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        # self.input_shape = input_shape\n",
    "        # self.conv_filters = conv_filters\n",
    "\n",
    "        self.vqvae = get_vqvae(input_shape, self.num_embeddings, conv_filters, self.latent_dim)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        checkpoint = tf.train.Checkpoint(optimizer=self.optimizer, model=self.vqvae)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(checkpoint, './checkpoints/Waveforms_with_VQVAE', checkpoint_name=\"checkpoint\", max_to_keep=5)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }\n",
    "        \n",
    "        \n",
    "conv_filters = 32\n",
    "latent_dim = 32\n",
    "n_batch = 1\n",
    "input_shape = (rate, embedding_size)\n",
    "\n",
    "epochs = 2\n",
    "n_train_size = 3\n",
    "# x_train = X[:n_train_size * n_batch]\n",
    "\n",
    "# trainer = VQVAETrainer(1, input_shape, conv_filters, latent_dim, embedding_size)\n",
    "\n",
    "# for epoch in  tqdm(range(epochs), \"Epoch\"):\n",
    "#     for i in tqdm(range(0, x_train.shape[0], n_batch), \"Batch\"):\n",
    "        \n",
    "#         step_x = np.array(x_train[i:n_batch])\n",
    "#         result = trainer.train_step(step_x)\n",
    "        \n",
    "#     # Checkpoints\n",
    "#     trainer.checkpoint_manager.save()\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit_vqvae(data, batch_size, epochs):\n",
    "    trainer = VQVAETrainer(0.2, input_shape, conv_filters, latent_dim, embedding_size)\n",
    "    \n",
    "    for _ in  range(epochs):\n",
    "        for i in tqdm(range(0, data.shape[0], batch_size), \"Batch\"):\n",
    "            step_x = data[i: i + batch_size]\n",
    "            result = trainer.train_step(step_x)\n",
    "            for key, value in result.items():\n",
    "                print(f\"{key}: {value}\", sep = \" | \", end=\"\")\n",
    "\n",
    "        # Checkpoints\n",
    "        trainer.checkpoint_manager.save()\n",
    "        \n",
    "        \n",
    "    return trainer.vqvae\n",
    "        \n",
    "x_train = X[:n_train_size * n_batch]   \n",
    "model = fit_vqvae(x_train, n_batch, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_vqvae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vqvae \u001b[38;5;241m=\u001b[39m \u001b[43mget_vqvae\u001b[49m(input_shape, embedding_size, conv_filters, latent_dim)\n\u001b[0;32m      3\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/Waveforms_with_VQVAE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_last_checkpoint\u001b[39m():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_vqvae' is not defined"
     ]
    }
   ],
   "source": [
    "vqvae = get_vqvae(input_shape, embedding_size, conv_filters, latent_dim)\n",
    "\n",
    "checkpoint_dir = \"./checkpoints/Waveforms_with_VQVAE\"\n",
    "def get_last_checkpoint():\n",
    "    pattern = r'checkpoint-(\\d+)\\.'\n",
    "    files = os.listdir(checkpoint_dir)\n",
    "    checkpoints = [int(re.match(pattern, file).group(1)) if re.match(pattern, file) else -1 for file in files if file.startswith(\"checkpoint\")]\n",
    "    return max(checkpoints)\n",
    "\n",
    "    \n",
    "\n",
    "vqvae.load_weights(f\"{checkpoint_dir}/checkpoint-{get_last_checkpoint()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
